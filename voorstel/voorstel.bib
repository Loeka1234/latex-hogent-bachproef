% Encoding: UTF-8

@Online{Bartley2023,
  author = {Kevin Bartley},
  date   = {2023-01-02},
  title  = {What is the difference between ETL and ELT?},
  url    = {https://rivery.io/blog/etl-vs-elt/},
}

@Article{Sreemathy2021,
  author       = {Sreemathy, J and Brindha, R and Nagalakshmi, M Selva and Suvekha, N and Ragul, N Karthick and Praveennandha, M},
  date         = {2021},
  journaltitle = {2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS)},
  title        = {Overview of etl tools and talend-data integration},
  url          = {https://intapi.sciendo.com/pdf/10.2478/picbe-2023-0182},
}

@Online{Azure2023,
  author = {Azure},
  date   = {2023-05-12},
  title  = {What is Azure Databricks?},
  url    = {https://learn.microsoft.com/en-us/azure/databricks/introduction/},
}

@InBook{Kromer2022,
  author    = {Kromer, Mark},
  booktitle = {Mapping Data Flows in Azure Data Factory: Building Scalable ETL Projects in the Microsoft Cloud},
  date      = {2022-08-26},
  title     = {Build Your First ETL Pipeline in ADF},
  doi       = {10.1007/978-1-4842-8612-8_4},
  isbn      = {978-1-4842-8612-8},
  pages     = {53--65},
  publisher = {Apress},
  url       = {https://doi.org/10.1007/978-1-4842-8612-8_4},
  abstract  = {Azure Data Factory (ADF) is a comprehensive Azure cloud service for data engineers to build complex data integration and ETL data pipelines. In the first few chapters, I've introduced cloud ETL concepts, Azure Data Factory basics, and the Mapping Data Flow design surface. Let's start to put some of these conceptual discussions into concrete project terms. Since the focus of this book is the low-code data transformation feature Mapping Data Flows, we'll take this chapter to walk through what it would take to build your first ADF pipeline that focuses on designing an ETL job using Mapping Data Flows inside an ADF pipeline to achieve a simple data analytics pattern, without writing any code.},
  address   = {Berkeley, CA},
  year      = {2022},
}

@InBook{Kromer2022a,
  author    = {Kromer, Mark},
  booktitle = {Mapping Data Flows in Azure Data Factory: Building Scalable ETL Projects in the Microsoft Cloud},
  date      = {2022-08-26},
  title     = {Introduction to Mapping Data Flows},
  doi       = {10.1007/978-1-4842-8612-8_3},
  isbn      = {978-1-4842-8612-8},
  pages     = {27--50},
  publisher = {Apress},
  url       = {https://doi.org/10.1007/978-1-4842-8612-8_3},
  abstract  = {Mapping Data Flows is the code-free data transformation feature in Azure Data Factory that allows data engineers to build powerful ETL jobs at scale. You can interactively design and test your data flow logic against live data and data samples while constructing a data transformation graph using the Mapping Data Flows designer UI. Then, you can operationalize your work as a Data Flow activity inside of an ADF pipeline. The Azure Integration Runtime is utilized for both debugging and pipeline executions of your data flows, while ADF manages an ephemeral and elastic Spark environment for you in a serverless manner. Throughout the next chapters in this book, I'll refer to the experience of building ETL logic as Mapping Data Flows, the pipeline activity to execute your work as Data Flows, and sometimes will shorten the full name to MDF.},
  address   = {Berkeley, CA},
  year      = {2022},
}

@InBook{Etaati2019,
  author    = {Etaati, Leila},
  booktitle = {Machine Learning with Microsoft Technologies: Selecting the Right Architecture and Tools for Your Project},
  date      = {2019-06-13},
  title     = {Azure Databricks},
  doi       = {10.1007/978-1-4842-3658-1_10},
  isbn      = {978-1-4842-3658-1},
  pages     = {159--171},
  publisher = {Apress},
  url       = {https://doi.org/10.1007/978-1-4842-3658-1_10},
  abstract  = {Databricks is an analytics service based on the Apache Spark open source project. Apache Spark is a batch processing and real time processing environment. Apache Spark is quite popular among data scientists because of its ability to analyze huge amounts of data, its streaming capabilities, graph computation, machine learning, and interactive queries engine. Spark provides in-memory cluster computing. One of the popular tools for big data analytics on Spark is Databricks. Databricks has been used for ingesting a significant amount of data, cleaning data, applying machine learning, and so forth. In February 2018, there was an integration between Microsoft Azure and Databricks that provides a better collaboration between data engineers, data scientists, and data analytics. This integration provides data science and data engineering teams with a fast, easy, and collaborative Spark-based platform in Azure [1]. Azure Databricks is a new platform for big data analytics and machine learning. The notebook in Azure Databricks enables data engineers, data scientists, and business analysts to collaborate using a single tool. This chapter gives an overview of what Azure Databricks is, the environment it inhabits, and its use in data science.},
  address   = {Berkeley, CA},
  year      = {2019},
}

@Article{Khan2024,
  author = {Khan, Bilal and Jan, Saifullah and Khan, Wahab and Chughtai, Muhammad Imran},
  date   = {2024-01-26},
  title  = {An Overview of ETL Techniques, Tools, Processes and Evaluations in Data Warehousing},
  url    = {https://cdn.techscience.cn/files/jbd/2024/TSP_JBD-6/TSP_JBD_46223/TSP_JBD_46223.pdf},
}

@Article{Jaiswal2022,
  author = {Nilesh Jaiswal},
  date   = {2022-12-21},
  title  = {Data Fusion Basics},
  url    = {https://medium.com/google-cloud/data-fusion-basic-concepts-c40b09efd695},
}

@Comment{jabref-meta: databaseType:biblatex;}
